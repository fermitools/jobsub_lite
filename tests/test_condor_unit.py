# pylint: disable=line-too-long
import copy
import os
import sys
import pytest

#
# we assume everwhere our current directory is in the package
# test area, so go ahead and cd there
#
os.chdir(os.path.dirname(__file__))


#
# import modules we need to test, since we chdir()ed, can use relative path
# unless we're testing installed, then use /opt/jobsub_lite/...
#
if os.environ.get("JOBSUB_TEST_INSTALLED", "0") == "1":
    sys.path.append("/opt/jobsub_lite/lib")
else:
    sys.path.append("../lib")
import condor

from test_unit import TestUnit


@pytest.fixture
def get_submit_file():
    tdir = f"/tmp/tst{os.getpid()}"
    if not os.path.exists(tdir):
        os.mkdir(tdir)
    filename = f"{tdir}/tst.sub"
    f = open(filename, "w")
    f.write(
        """
# generated by jobsub_lite
#
universe           = vanilla
executable         = /bin/true
arguments          =

output             = lookaround.xx.$(Cluster).$(Process).out
error              = lookaround.xx.$(Cluster).$(Process).err
log                = lookaround.xx.$(Cluster).$(Process).log
environment        = CLUSTER=$(Cluster);PROCESS=$(Process);CONDOR_TMP=;BEARER_TOKEN_FILE=.condor_creds/{group}.use;CONDOR_EXEC=/tmp;DAGMANJOBID=$(DAGManJobId);GRID_USER={user};JOBSUBJOBID=$(CLUSTER).$(PROCESS)@schedd.example.com;EXPERIMENT={group};SAM_EXPERIMENT=samdev
rank               = Mips / 2 + Memory
job_lease_duration = 3600
notification       = Never
transfer_output    = True
transfer_error     = True
transfer_executable= True
when_to_transfer_output = ON_EXIT_OR_EVICT
transfer_output_files = .empty_file
request_memory = 2048.0
request_disk = 102400.0KB
+JobsubClientDN=""
+JobsubClientIpAddress="131.225.67.71"
+JobsubServerVersion="lite_v1_0"
+JobsubClientVersion="lite_v1_0"
+JobsubClientKerberosPrincipal=""
+JOB_EXPECTED_MAX_LIFETIME = 28800.0
notify_user = {user}@fnal.gov
+AccountingGroup = "group_{group}.{user}"
+Jobsub_Group="{group}"
+JobsubJobId="$(CLUSTER).$(PROCESS)@schedd.example.com"
+Drain = False
+GeneratedBy = "schedd.example.com"

+DESIRED_usage_model="OPPORTUNISTIC,DEDICATED"

requirements  = target.machine =!= MachineAttrMachine1 && target.machine =!= MachineAttrMachine2  && (isUndefined(DesiredOS) || stringListsIntersect(toUpper(DesiredOS),IFOS_installed)) && (stringListsIntersect(toUpper(target.HAS_usage_model, toUpper(my.DESIRED_usage_model))))

#
# this is supposed to get us output even if jobs are held(?)
#
+SpoolOnEvict = false
#
#
#
use_oauth_services = {group}

queue 1
    """.format(
            user=os.environ["USER"], group=TestUnit.test_group
        )
    )
    f.close()
    return filename


@pytest.fixture
def get_dag_file(get_submit_file):
    tdir = f"/tmp/tst{os.getpid()}"
    filename = f"{tdir}/tst.dag"
    if not os.path.exists(tdir):
        os.mkdir(tdir)
    f = open(filename, "w")
    f.write(
        """
DOT dataset.dag.dot UPDATE
JOB SAM_START {cmdfile}

JOB WORKER_1 {cmdfile}
PARENT SAM_START CHILD WORKER_1
PARENT WORKER_1 CHILD SAM_END

JOB WORKER_2 {cmdfile}
PARENT SAM_START CHILD WORKER_2
PARENT WORKER_2 CHILD SAM_END

JOB SAM_END {cmdfile}
    """.format(
            cmdfile=get_submit_file
        )
    )
    f.close()
    return filename


class TestCondorUnit:
    """
    Use with pytest... unit tests for ../lib/*.py
    """

    # lib/condor.py routines...

    @pytest.mark.unit
    def test_get_schedd_1(self):
        """make sure we get our test schedd back with test_vargs"""
        schedd = condor.get_schedd(TestUnit.test_vargs)
        print("got schedd: {0}".format(schedd))
        print("schedd name: {0}".format(schedd["Name"]))
        assert schedd["Name"] == TestUnit.test_schedd

    @pytest.mark.unit
    def test_get_schedd_list_cache(self, capsys):
        """Call get_schedd_list twice. The first time, we should get the
        schedd ads from the collector.  The second time, it should come
        from cache.  Call it a third time, and force the refresh."""
        vargs = TestUnit.test_vargs
        vargs["verbose"] = 2

        # First time:  We should query collector , but if other tests have
        # run before us that call, we wouldn't, so pass the refresh flag
        condor.get_schedd_list(vargs, refresh_schedd_ads=True)
        captured = capsys.readouterr()
        assert "Querying condor collector" in captured.out

        # Second time: We should query cache
        condor.get_schedd_list(vargs)
        captured = capsys.readouterr()
        assert "Using cached schedd ads - NOT querying condor collector" in captured.out

        # Third time: Force re-querying of collector
        condor.get_schedd_list(vargs, refresh_schedd_ads=True)
        captured = capsys.readouterr()
        assert "Querying condor collector" in captured.out

    @pytest.mark.unit
    def test_load_submit_file_1(self, get_submit_file):
        """make sure load_submit_file result has bits of the submit file"""
        res = condor.load_submit_file(get_submit_file)
        assert str(res[0]).find("universe = vanilla") >= 0
        assert str(res[0]).find("executable = /bin/true") >= 0

    @pytest.mark.unit
    def test_submit_1(self, get_submit_file, needs_credentials):
        """actually submit a job with condor_submit"""
        res = condor.submit(get_submit_file, TestUnit.test_vargs, TestUnit.test_schedd)
        print("got: ", res)
        assert res

    @pytest.mark.unit
    def test_submit_sec_cred_storer_predefined_verbose(
        self, get_submit_file, needs_credentials, capsys, monkeypatch
    ):
        """Make sure that if we have _condor_SEC_CREDENTIAL_STORER defined before we run
        condor_submit, we coerce it to the correct value.  This is how all POMS submissions expect
        to operate.  This uses verbose mode so we can actually check the output."""
        monkeypatch.setenv("_condor_SEC_CREDENTIAL_STORER", "/bin/true")
        vargs = copy.deepcopy(TestUnit.test_vargs)
        vargs["verbose"] = 1
        res = condor.submit(get_submit_file, vargs, TestUnit.test_schedd)
        print("got: ", res)
        assert res  # Did submission succeed?

        jl_condor_vault_storer_path = os.path.join(
            os.path.dirname(os.path.dirname(__file__)), "bin", "condor_vault_storer"
        )
        captured = capsys.readouterr()
        # Make sure our submission used the correct _condor_SEC_CREDENTIAL_STORER
        assert (
            f'_condor_SEC_CREDENTIAL_STORER="{jl_condor_vault_storer_path} -v"'
            in captured.out
        )
        # ...but that the overall environment was unchanged
        assert os.getenv("_condor_SEC_CREDENTIAL_STORER") == "/bin/true"

    @pytest.mark.unit
    def test_submit_sec_cred_storer_predefined_debug(
        self, get_submit_file, needs_credentials, capsys, monkeypatch
    ):
        """Make sure that if we have _condor_SEC_CREDENTIAL_STORER defined before we run
        condor_submit, we coerce it to the correct value.  This is how all POMS submissions expect
        to operate.  This uses debug mode"""
        monkeypatch.setenv("_condor_SEC_CREDENTIAL_STORER", "/bin/true")
        vargs = copy.deepcopy(TestUnit.test_vargs)
        vargs["verbose"] = 2  # debug mode
        res = condor.submit(get_submit_file, vargs, TestUnit.test_schedd)
        print("got: ", res)
        assert res  # Did submission succeed?

        jl_condor_vault_storer_path = os.path.join(
            os.path.dirname(os.path.dirname(__file__)), "bin", "condor_vault_storer"
        )
        captured = capsys.readouterr()
        # Make sure our submission used the correct _condor_SEC_CREDENTIAL_STORER
        assert (
            f'_condor_SEC_CREDENTIAL_STORER="{jl_condor_vault_storer_path} -d"'
            in captured.out
        )
        # ...but that the overall environment was unchanged
        assert os.getenv("_condor_SEC_CREDENTIAL_STORER") == "/bin/true"

    @pytest.mark.unit
    def test_submit_too_many_procs(self, get_submit_file, needs_credentials, capsys):
        """Submit a job with too many procs being submitted.  Note that this test takes
        a long time since we're trying to submit TOO_MANY_PROCS jobs"""
        TOO_MANY_PROCS = 50000
        submit_file = get_submit_file
        # Need to swap out the queue value
        with open(submit_file, "r+") as f:
            lines = (line for line in f.readlines())
            edited_lines = [
                f"queue {TOO_MANY_PROCS}" if "queue" in line else line for line in lines
            ]
            f.writelines([edited_line + "\n" for edited_line in edited_lines])
        condor.submit(submit_file, TestUnit.test_vargs, TestUnit.test_schedd)
        captured = capsys.readouterr()
        assert (
            "MAX_JOBS_PER_SUBMISSION limits the number of jobs allowed in a submission"
            in captured.err
        )

    @pytest.mark.unit
    def test_submit_dag_1(self, get_dag_file, needs_credentials):
        """actually submit a dag with condor_submit_dag"""
        # XXX fix me
        x = TestUnit.test_vargs.copy()
        x["no_submit"] = True
        res = condor.submit_dag(get_dag_file, x, TestUnit.test_schedd, cmd_args=[])
        # submit returns False with no_submit on...
        assert not res


class TestJob:
    @pytest.mark.unit
    def test_job(self):
        jid = "123.456@foo.example.com"
        j = condor.Job(jid)
        assert j.id == jid
        assert j.seq == 123
        assert j.proc == 456
        assert j.schedd == "foo.example.com"
        assert not j.cluster
        assert str(j) == jid
        assert j._constraint() == "ClusterId==123 && ProcId==456"

    @pytest.mark.unit
    def test_cluster(self):
        jid = "123@foo.example.com"
        j = condor.Job(jid)
        assert j.id == jid
        assert j.seq == 123
        assert j.proc == 0
        assert j.schedd == "foo.example.com"
        assert j.cluster
        assert str(j) == jid
        assert j._constraint() == "ClusterId==123"

    @pytest.mark.unit
    def test_bad_jobs(self):
        for jid in ["foo", "foo@example.com" "foo.bar@example.com"]:
            try:
                j = condor.Job(jid)
            except condor.JobIdError:
                pass
            else:
                raise Exception(f"job id {jid} should have raised JobIdError")
