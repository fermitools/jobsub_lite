#!/usr/bin/python3 -I

#
# jobsub_submit -- wrapper for condor_submit
# COPYRIGHT 2021 FERMI NATIONAL ACCELERATOR LABORATORY
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
#
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""" submit command for jobsub layer over condor """
# pylint: disable=wrong-import-position,wrong-import-order,import-error
import glob
import hashlib
import os
import os.path
import sys
from typing import Union, List, Dict, Any

#
# we are in prefix/bin/jobsub_submit, so find our prefix
#
PREFIX = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

#
# find parts we need in package management
# this means we don't need fancy ups dependencies..
#
sys.path.append(os.path.join(PREFIX, "lib"))
from packages import pkg_find

pkg_find("jinja")
import jinja2 as jinja

#
# import our local parts
#
from get_parser import get_parser
from condor import get_schedd, submit, submit_dag
from dagnabbit import parse_dagnabbit
from tarfiles import do_tarballs
from utils import set_extras_n_fix_units, cleanup
from creds import get_creds
from token_mods import get_job_scopes, use_token_copy
import version


def get_basefiles(dlist: List[str]) -> List[str]:
    """get basename of files in directory"""
    res = []
    for d in dlist:
        flist = glob.glob(f"{d}/*")
        for f in flist:
            res.append(os.path.basename(f))
    return res


def render_files(
    srcdir: str, values: Dict[str, Any], dest: str, dlist: Union[None, List[str]] = None
):
    """use jinja to render the templates from srcdir into the dest directory
    using values dict for substitutions
    """
    if values.get("verbose", 0) > 0:
        print(f"trying to render files from {srcdir}\n")

    if dlist is None:
        dlist = [srcdir]
    values["transfer_files"] = get_basefiles(dlist)

    jinja_env = jinja.Environment(
        loader=jinja.FileSystemLoader(srcdir), undefined=jinja.StrictUndefined
    )
    jinja_env.filters["basename"] = os.path.basename
    flist = glob.glob(f"{srcdir}/*")

    # add destination dir to values for template
    values["cwd"] = dest

    for f in flist:
        if values["verbose"] > 0:
            print(f"rendering: {f}")
        bf = os.path.basename(f)
        rendered_file = os.path.join(dest, bf)
        try:
            with open(rendered_file, "w", encoding="UTF-8") as of:
                of.write(jinja_env.get_template(bf).render(**values))
        except jinja.exceptions.UndefinedError as e:
            err = f"""Cannot render template file {f} due to undefined template variables.
{e}
Please open a ticket to the Service Desk and include this error message
in its entirety.
"""
            print(err)
            raise
        if rendered_file.endswith(".sh"):
            os.chmod(rendered_file, 0o755)
        else:
            if values.get("verbose", 0) > 0:
                print(f"Created file {rendered_file}")


def do_dataset_defaults(varg: Dict[str, Any]) -> None:
    """
    make sure to pass appropriate SAM_* environment variables if we
    are doing datasets.  Pick a SAM_PROJECT name if we don't have one.
    """
    have_project = False
    have_dataset = False
    have_station = False
    have_user = False
    have_group = False
    experiment = varg["group"]

    if varg["project_name"]:
        varg["environment"].append(f"SAM_PROJECT={varg['project_name']}")

    for e in varg["environment"]:
        pos = e.find("=")
        if e[:pos] == "SAM_PROJECT":
            have_project = True
        if e[:pos] == "SAM_DATASET":
            have_dataset = True
        if e[:pos] == "SAM_STATION":
            have_station = True
        if e[:pos] == "SAM_USER":
            have_user = True
        if e[:pos] == "SAM_GROUP":
            have_group = True
        if e[:pos] == "SAM_EXPERIMENT":
            experiment = e[pos + 1 :]

    if not have_project:
        # if not, grab from the environment, or use dataset_$USER_$uuid
        varg["environment"].append(
            "SAM_PROJECT="
            + os.environ.get(
                "SAM_PROJECT",
                f'{varg["dataset_definition"]}_{os.environ.get("USER", "")}_{varg["uuid"]}',
            )
        )
    if not have_dataset:
        varg["environment"].append(f"SAM_DATASET={varg['dataset_definition']}")
    if not have_station:
        varg["environment"].append(f"SAM_STATION={experiment}")
    if not have_user:
        varg["environment"].append(f"SAM_USER={os.environ['USER']}")
    if not have_group:
        varg["environment"].append(f"SAM_GROUP={experiment}")


def main():
    """script mainline:
    - parse args
    - get credentials
    - handle tarfile options
    - set added values from environment, etc.
    - convert/render template files to submission files
    - launch
    """
    # pylint: disable=too-many-statements
    parser = get_parser()
    args = parser.parse_args()

    if args.version:
        print(f"jobsub_lite version {version.__version__}")
        exit()

    if args.support_email:
        print(f"Email {version.__email__} for help.")
        exit()

    if os.environ.get("GROUP", None) is None:
        raise SystemExit(f"{sys.argv[0]} needs -G group or $GROUP in the environment.")

    # While we're running in hybrid proxy/token mode, force us to get a new proxy every time we submit
    # Eventually, this arg and its support in the underlying libraries should be removed
    args.force_proxy = True

    varg = vars(args)

    proxy, token = get_creds(varg)

    if args.verbose:
        print(f"proxy is : {proxy}")
        print(f"token is : {token}")

    if args.debug:
        sys.stderr.write(f"varg: {repr(varg)}\n")

    do_tarballs(args)

    schedd_add = get_schedd(varg)
    schedd_name = schedd_add.eval("Machine")

    #
    # We work on a copy of our bearer token because
    # condor_vault_storer is going to overwrite it with a token with the weakened scope
    # then get our weakened job scopes, then
    # set the "oauth handle" we're going to use to a hash of our job scopes, so we have a
    # different handle for different scopes.  This makes condor
    # a) store the token as, say "mu2e_830a3a3188.use" and
    # b) refresh it there and
    # c) pass it to the jobs that way.
    # That way if they submit another job with, say,  an additional
    # storage.create:/mu2e/my/output/dir  they will store that token in a file with a
    # different hash, and *that* will get sent to *those* jobs.
    # If they submit another job with these *same* permissions, they will *share* the
    # token filename the conor_vault_credmon will only refresh it once for both (or all
    # three, etc.) submissions, and push that token to all the jobs with that same handle.
    #
    token = use_token_copy(token)
    varg["job_scope"] = " ".join(
        get_job_scopes(token, args.need_storage_modify, args.need_scope)
    )
    m = hashlib.sha256()
    m.update(varg["job_scope"].encode())
    varg["oauth_handle"] = m.hexdigest()[:10]

    set_extras_n_fix_units(varg, schedd_name, proxy, token)
    submitdir = varg["outdir"]

    # if proxy:
    #    proxy_dest=os.path.join(submitdir,os.path.basename(proxy))
    #    shutil.copyfile(proxy, proxy_dest)
    #    varg["proxy"] = proxy_dest
    # if token:
    #    token_dest=os.path.join(submitdir,os.path.basename(token))
    #    shutil.copyfile(token, token_dest)
    #    varg["token"] = token_dest

    if args.dag:
        varg["is_dag"] = True
        d1 = os.path.join(PREFIX, "templates", "simple")
        d2 = os.path.join(PREFIX, "templates", "dag")
        parse_dagnabbit(d1, varg, submitdir, schedd_name, varg["verbose"] > 1)
        render_files(d2, varg, submitdir, dlist=[d2, submitdir])
        if not varg.get("no_submit", False):
            os.chdir(varg["submitdir"])
            submit_dag(os.path.join(submitdir, "dag.dag"), varg, schedd_name)
    elif args.dataset_definition:
        varg["is_dag"] = True
        do_dataset_defaults(varg)
        d1 = os.path.join(PREFIX, "templates", "dataset_dag")
        d2 = f"{PREFIX}/templates/simple"
        # so we render the simple area (d2) with -N 1 because
        # we are making a loop of 1..N in th dataset_dag area
        # otherwise we get N submissions of N jobs -> N^2 jobs...
        saveN = varg["N"]
        varg["N"] = "1"
        render_files(d2, varg, submitdir, dlist=[d1, d2])
        varg["N"] = saveN
        render_files(d1, varg, submitdir, dlist=[d1, d2, submitdir])
        if not varg.get("no_submit", False):
            os.chdir(varg["submitdir"])
            submit_dag(os.path.join(submitdir, "dataset.dag"), varg, schedd_name)
    elif args.maxConcurrent:
        varg["is_dag"] = True
        d1 = os.path.join(PREFIX, "templates", "maxconcurrent_dag")
        d2 = os.path.join(PREFIX, "templates", "simple")
        render_files(d2, varg, submitdir, dlist=[d1, d2])
        render_files(d1, varg, submitdir, dlist=[d1, d2, varg["dest"]])
        if not varg.get("no_submit", False):
            os.chdir(varg["submitdir"])
            submit_dag(os.path.join(submitdir, "maxconcurrent.dag"), varg, schedd_name)
    else:
        varg["is_dag"] = False
        d = f"{PREFIX}/templates/simple"
        render_files(d, varg, submitdir)
        if not varg.get("no_submit", False):
            os.chdir(varg["submitdir"])
            submit(os.path.join(submitdir, "simple.cmd"), varg, schedd_name)

    if varg.get("no_submit", False):
        print(f"Submission files are in: {varg['submitdir']}")
    else:
        cleanup(varg)


if __name__ == "__main__":
    main()
